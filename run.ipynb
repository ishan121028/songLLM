{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# songLLM : Instructions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This contains the instructions that we can follow to train an LLM (over spotify's million songs data) as well as serve it over an API so we can request it a response to a prompt by cURL command.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The directory consists of a train.py file, which we use to train it over the dataset.\n",
    "The dataset for this is in the directory \"/dataset/spotify_millsongdata.csv\" in a csv format. We use only the text column from the csv file to train our model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we have to run the train the file, and give the parameters of the file path used for the dataset in the 'fp' argument and the learning rate as 'lr'."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run this command to run the train file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --fp dataset/spotify_millsongdata.csv --lr 5e-4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our file would start training now, and the weights and configuration file would be saved in the /out/songLLM directory. We can move forward to doing inference on the model using the server.py file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the server using this command: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m uvicorn server:app --reload"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have run our server, we can request an output from it using the curl command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!set json={\"text\": \"NimbleBox!\", \"max_length\": 1000}\n",
    "!curl -i -X POST -H \"Content-Type: application/json\" -d \"%json:\"=\\\"%\" http://127.0.0.1:8000/generatetext"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would get a json file in return which would consist of our generated text."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NimbleBox  \n",
    "And I don't need so long  \n",
    "With this magic long from the side of love  \n",
    "I've learned more like a friend sead  \n",
    "Me sea, don't need to tell me a cryin'  \n",
    "I could stay this word who I've changed to die  \n",
    "There's only on, my let this workin'  \n",
    "It's only here with the man on the rive  \n",
    "  \n",
    "I thought I've found it too down and I look\n",
    "\n",
    "\n",
    "All your friends fading back to find  \n",
    "Oh, your brother's true  \n",
    "Your love is she haven't let you her cause  \n",
    "At the truth  \n",
    "Oh-oh, tonight  \n",
    "  \n",
    "I got to tell you  \n",
    "I got to be crazy  \n",
    "Babe to break it on your wires  \n",
    "There you with a batterflower  \n",
    "Stop until the little time  \n",
    "Babe I'm never finally been all nothing  \n",
    "I'm alone in the nighteek, too stunna loads  \n",
    "Shit into your house in the night  \n",
    "And looking for a woman chance  \n",
    "I know that a woman that you know  \n",
    "She likes so many thing  \n",
    "But you can look in my cleatel hondoor  \n",
    "I live in my living way back the heat  \n",
    "Is sing me and myself  \n",
    "Seeks like a big body banging ba"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next comming to the part of using multithreading for stress testing as well as making our model work fast (using CLI).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stress.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My approach for the stress test part was to use multithreading to create some number of threads and each thread with some number of junk requests, these parameters can be given as a input by CLI easily. After giving the input of threads as well as requests, the program bombards the given url with requests. \n",
    "\n",
    "I successfully bottlenecked the server with about 10 threads and 200 requests which means I recieved no requests for over 5-10 minutes for such a input.\n",
    "\n",
    "This command can be used to stress thread the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python stress_test.py --url http://127.0.0.1:8000/generatetext --threads 10 --requests 10 --max_length 500"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normal_inference.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this file I used argparse to take inputs of the requests (multiple at same time) and send those requests one by one to the server.\n",
    "I sent these requests to the server one by one: \"I\" \"have\" \"applied\" \"for\" \"NimbleBox\" \"Internship\".\n",
    "I got the response in about 39 seconds.\n",
    "The max throughput from the system (rps) was around 6.5 seconds.\n",
    "We can do normal inference using this particular command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python normal_inference.py --url http://127.0.0.1:8000/generatetext --messages \"I\" \"have\" \"applied\" \"for\" \"NimbleBox\" \"Internship\" --max_length 500"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multi_inference.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this file I wrapped multiple requests into the same number of threads and bombarded the server with requsts at the same time, and was able to achieve a much higher throughput (rps) of around 4.5 seconds.\n",
    "We can do multi inference using this command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python multi_inference.py --url http://127.0.0.1:8000/generatetext --messages \"I\" \"have\" \"applied\" \"for\" \"NimbleBox\" \"Internship\" --max_length 500"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would like to say that I learned a lot in this particular assignment/challenge, such that I didn't knew to create APIs before this, nor did I used them a lot. However, for this particular assignment I was not only able to create one API but was also able to deploy a server, send requests to it, and also was able to do inference (multithreaded also) from the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
